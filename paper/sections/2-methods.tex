\section{Methods}
\label{sec:methods}

\subsection{Framing the Problem}

The goal of our analysis is to develop a model that predicts the winners of in-progress basketball games. Precisely, we would like our model to take as input a sequence of events $\mathbf X_i = (\mathbf x_t)_{t \leq T_i}$ and output a sequence $\mathbf P_i = (p_t)_{t \leq T}$ estimating the probability that the home team wins at each time step. This is a non-traditional statistics problem, and our analysis faces several challenges.

First, the input to our classification has variable dimension. Although the individual events have constant dimension (69), the number of events per sequence varies widely. For example, making a prediction at the end of the first quarter may require only looking at 100 or so events, whereas a prediction with two minutes left in the fourth quarter may require looking at four or five times that many. This renders classic binary classification techniques like logistic regression useless \cite[\S 4.4]{ESL}.

This sequential aspect of our data makes our problem smell like it could benefit from a time-series approach \cite[ch. 4, 5]{TSA}, however classic time-series techniques like ARIMA are not appropriate for a few reasons:
\begin{enumerate}
	\item ARIMA models are based on fixed-size moving windows and can only look a specified distance into the past, when our problem requires the prediction to be a function of the \emph{entire} past sequence.
	\item ARIMA (and most other) models attempt to predict the next value in a sequence. For us, this would correspond to predicting the next event in the game, which is not the goal of our analysis.
\end{enumerate}

The second big challenge is that our data consists mainly of categorical variables with few numeric columns, making techniques like support vector classifiers less appropriate \cite[\S 9]{ISL}. A popular approach for categorically-heavy data is to use a tree-based method like random forests, often combined with an ensemble tools like bagging and boosting \cite[\S 9, \S 10]{ESL}. However, it was not clear how to adapt these methods in order to handle our sequential input.

In order to handle these two issues, we resort to using neural networks.

\subsection{Neural Networks}

It's hard to define exactly what a neural network is: the term is used broadly to describe a large class of model characterized mostly by the method used to estimate the models' parameters: gradient descent via the backpropagation algorithm. In this paper, we focus on two specific types of neural networks: the multilayer perceptron (MLP) \cite[\S 13.2.2]{PML} and the recurrent neural network (RNN) \cite[\S 15.2]{PML}. MLPs are a flexible classification and regression model that can handle our categorically-heavy input; RNNs enable our model to handle sequences.

At a high level, our approach for predicting game winners centers around maintaining a vector $\mathbf h_t$ that represents the ``state of the game'' at a time $t$. At each time step, we use a recurrent neural network to update $\mathbf h_t$ using the previous state $\mathbf h_{t-1}$ and the current event $\mathbf x_t$. We then use this state $\mathbf h_t$ to derive an estimate for the home team win probability using a multilayer perceptron. This approach is developed in detail in \autoref{sec:complete-model}, but first, we describe the necessary pieces.

This is \emph{not} a comprehensive treatment of MLPs, RNNs, or deep learning in general. There are many more types of neural networks, each of which is well-suited for a specific task. A few examples are convolutional neural networks (which are commonly used for image-related tasks) \cite[\S 14]{PML}, transformers (which power natural-language models like GPT) \cite{attention-is-all-you-need}, and generative adversarial networks (which have found recent fame due their ability to generate deep-fakes) \cite[19.3.6.2]{PML}. Chapter 11 of \textcite{ESL} and Part III of \textcite{PML} provide excellent and statistically-motivated introductions to the subject.

\subsection{The Multi-layer Perceptron}

In this section, we describe the simplest non-trivial neural network known as a \emph{multi-layer perceptron} or \emph{feedforward neural network} \cite[\S 10.1]{ISL}, \cite[\S 11.3]{ESL}, \cite[13.2]{PML}. Very loosely, multilayer perceptrons are non-linear sandwiches on slices of linear bread; we use them as basic building blocks in several places for our final model.

More precisely, in order to model a function $f : \R^n \to \R^m$, we introduce an intermediate space $\R^k$ called the \emph{hidden layer}. We first map our inputs $\mathbf z$ into this intermediate space via a linear function $f_1(\mathbf z) = A \mathbf z + \mathbf b$ where $A$ is a $k \times n$ matrix and $\mathbf b$ is a $k$-dimensional vector. We then apply a non-linear function $\sigma$ element-wise to our values in the intermediate space $\R^k$ before mapping back down to our output space $\R^m$ via a second linear transformation $f_2(\mathbf z) = C \mathbf z$ where $C$ is an $m \times k$ matrix. In total, the multi-layer perceptron model equation is
\begin{equation}
	\label{eqn:mlp}
	f(\mathbf z)
	= f_2 \of \sigma \of f_1(\mathbf z)
	= C \, \sigma(A\mathbf z + \mathbf z).
\end{equation}
The model's parameters are entries of the vector $\mathbf b$ and the matrices $A$ and $C$.
The dimension of the hidden space $k$ and the non-linear function $\sigma$ are model hyperparameters and are chosen in advance.

In the world of deep learning, we typically refer $A$ and $C$ as \emph{weights} and $\mathbf b$ as \emph{bias}. The function $\sigma$ is known as the \emph{activation function}. Historically, the most popular choices for $\sigma$ have been the sigmoid function
\begin{equation}
	\sigma(x) = \frac{e^x}{1 + e^x}
\end{equation}
and the hyperbolic tangent, $\tanh$ \cite[\S 10.1]{ISL}. More recently the ``rectified linear unit'' function
\begin{equation}
	{\rm ReLu}(x) = \max(x, 0)
\end{equation}
has become popular because it and its derivative are cheap and easy to compute \cite[\S 10.5]{ISL}, \cite[\S 13.3.2]{PML}. (The following section will reveal why this is relevant.) These activation functions are always applied elementwise to their vector inputs, and plots of the three popular functions described above are given in \autoref{fig:activation-functions}.

\begin{figure}[h]
	\begin{tikzpicture}[baseline]
		\datavisualization[
		scientific axes=clean,
		y axis=grid,
		visualize as smooth line/.list={sin,cos,tan},
		style sheet=strong colors,
		% style sheet=vary dashing,
		sin={label in legend={text={$\sigma(x) = {e^x}/\paren{1+e^x}$}}},
		cos={label in legend={text={$\tanh(x)$}}},
		tan={label in legend={text={$\mathrm{ReLu}(x) = \max(0, x)$}}},
		data/format=function
		]
		data [set=sin] {
				var x : interval [-3:3];
				func y = pow(2.718281, \value x) / (1 + pow(2.718281, \value x));
			}
		data [set=cos] {
				var x : interval [-3:3];
				func y = tanh(\value x);
			}
		data [set=tan] {
				var x : interval [-3:1.5];
				func y = max(0, \value x);
			};
	\end{tikzpicture}
	\caption{The three most popular activation functions: the sigmoid function, the hyperbolic tangent function, and the rectified linear unit.}
	\label{fig:activation-functions}
\end{figure}

\begin{figure}[h]
	\centering
	\begin{tikzpicture}
		\graph[
		math nodes,
		layered layout, grow=right, level sep=6em, sibling sep=3em,
		edges={->, gray}, nodes={inner sep = 1em}
		]{
		subgraph I_n [V={z_3, z_2, z_1}, name=input] -- [complete bipartite]
		subgraph I_n [V={u_4, u_3, u_2, u_1}, name=hidden] (hidden) -- [complete bipartite]
		subgraph I_n [V={y}, name=output]
		};

		\node[above = 1em of hidden u_1] (hidden label) {\underline{Hidden layer}};
		\node[left = of hidden label] {\underline{Input layer}};
		\node[right = of hidden label] {\underline{Output layer}};
	\end{tikzpicture}

	\caption{A multi-layer perceptron with input dimension $3$, hidden dimension $4$, and output dimension $1$. We first map our input $\mathbf z = (z_1, z_2, z_3)$ into a ``hidden layer'' via a linear transformation. In the internal hidden layer, we apply a non-linear function element-wise before applying another linear transformation to map down to our output $y$. Mathematically, the ``neurons'' $\mathbf u = (u_1, \ldots, u_4)$ in the hidden layer take on the values $\mathbf u = \sigma(A\mathbf z + \mathbf b)$ and the ``neuron'' in the output layer takes the value $y = C\mathbf u$. The ``weights'' (the entries of $A$ and $C$) and ``biases'' (the entries of $\mathbf b$) are the model's parameters.}
	\label{fig:mlp-diagram}
\end{figure}

This simple model is what is known as a \emph{universal function approximator}. In layman's terms, this means that for any function $g : \R^n \to \R^m$, there exist choices for $k$, $A$, $\mathbf b$ and $C$ such that the multi-layer perceptron can approximate $g$ to any desired level of precision. More mathematically, for any continuous $g : K \to \R^m$ where $K \subseteq R^n$ is compact, there exists a sequence of functions $g_i$ of the form in \autoref{eqn:mlp} that uniformly converges to $g$. This property is true regardless of the choice of activation function, with the only condition being that $\sigma$ not be polynomial \cite[\S 13.2.5]{PML}.

Although this universal flexibility in theory only requires a single hidden layer, the above theorem places no bound on $k$---the hidden layer's dimension. In practice, one can (and often does) increase the network's ``depth'' by including multiple hidden layers in order to limit the networks ``width'' \cite[\S 13.2.5]{PML}, \cite[10.2]{ISL}. (This is where the phrase deep learning comes from.) The construction extends quite naturally from the setting with a single hidden layer: between each layer, we apply a linear transformation. Within each layer, we apply an element-wise non-linearity.

The phrase ``neural network'' makes a bit of sense when the input, hidden, and output spaces are laid out as in \autoref{fig:mlp-diagram}. Here, the elements of each space are thought of as ``neurons'' with the weights representing the strength of each neural connection.
This connection with the human brain was apparently the inspiration for the conception of the multilayer perceptron in 1958, however the terms ``perceptron'' and ``neural network'' often receive criticism for exaggerating what is at best a tenuous connection \cite[\S 13.2.7]{PML}.

\subsection{Recurrent Neural Networks}

\begin{figure}[t]
	\centering
	\begin{tikzpicture}[
			node distance = 1.5em and 4em,
			->,
			module/.style={ draw, rounded corners,
					inner sep=10pt, outer sep=5pt},
			every edge quotes/.style={fill=white, font=\small},
		]

		\node[module] (rnn1) {RNN};
		\node[module, right = of rnn1] (rnn2) {RNN};
		\node[module, right = of rnn2] (rnn3) {RNN};

		\node[left = of rnn1] (h0) {};
		\node[right = of rnn3] (h1) {};

		\node[above = of rnn1] (x1) {$\mathbf x_t$};
		\node[above = of rnn2] (x2) {$\mathbf x_{t+1}$};
		\node[above = of rnn3] (x3) {$\mathbf x_{t+2}$};

		\draw[->] (x1) -- (rnn1);
		\draw[->] (x2) -- (rnn2);
		\draw[->] (x3) -- (rnn3);
		\draw[->] (h0) edge["$\mathbf h_{t-1}$"] (rnn1);
		\draw[->] (rnn1) edge["$\mathbf h_t$"] (rnn2);
		\draw[->] (rnn2) edge["$\mathbf h_{t+1}$"] (rnn3);
		\draw[->] (rnn3) edge["$\mathbf h_{t+2}$"] (h1);
	\end{tikzpicture}

	\caption{A simple recurrent neural network.}
	\label{fig:simple-rnn}
\end{figure}

Although the multilayer perceptron described in the previous section is quite powerful, it has a notable drawback that prevents us from applying it directly to our problem: MLPs can only handle input of a fixed dimension. Recall that our goal is to develop a model that takes in an in-progress game represented as a sequence of the form $(\mathbf x_1, \ldots, \mathbf x_t)$ and outputs a probability $p_t$ that the home team will win. Although each event $\mathbf x_t$ is of a constant dimension, the win probability depends on much more than just a single event. In fact, $p_t$ depends on \emph{all} the preceding events $\mathbf x_1, \ldots, \mathbf x_t$.

In order to enable past events to influence present predictions, one common approach is to maintain a ``hidden state'' vector $\mathbf h_t$ that one can think of as capturing the essence of the sequence up to time $t$. We continually update $\mathbf h_t$ with new events as they occur, as is shown in \autoref{fig:simple-rnn}.
% To derive the predicted win probability, we use a multilayer perceptron $\mathbf h_t$.
This approach to sequence modeling is known as a \emph{recurrent neural network} because the hidden state is updated recursively: $\mathbf h_t$ is a function of $\mathbf x_t$ and $\mathbf h_{t-1}$ \cite[\S 10.5]{ISL}, \cite[\S 15]{PML}.

The RNN internals are similar to the multilayer perceptron: we update the hidden state via a non-linear function applied to a linear combination of $\mathbf x_t$ and the previous hidden state $\mathbf h_{t-1}$. Precisely,
\begin{equation}
	\label{eqn:elman-rnn}
	h_t = \tanh (A \mathbf x_t + B \mathbf h_{t-1} + \mathbf c)
\end{equation}
where $\sigma$ is some non-linear function, $\mathbf c$ is a vector, and $A$ and $B$ are matrices. We use the hyperbolic tangent because that is the default option in the PyTorch package we use to implement our model \cite{pytorch}. We also use two other forms of recurrent neural networks known as Gated Recurrent Units \cite[\S 15.2.7.1]{PML} and Long Short Term Memory \cite[\S 10.5.1]{ISL}, \cite[\S 15.2.7.2]{PML}. These employ complex internal logic that enables events further in the past to have more influence on the current hidden state \cite[\S 15.2.6]{PML}. Explaining the precise internals of these two RNN types is outside the scope of this paper. See \textcite[\S 15.2.7]{PML} for a complete exposition.

\subsection{The Complete Sport Sequence Model}
\label{sec:complete-model}

With all the pieces built up, we now describe our complete model.

Each event in a game sequence $\mathbf X_i = (\mathbf x_t)$ is first fed into a multilayer perceptron before being passed into a recurrent layer to generate the hidden game state $\mathbf h_t$. This game state is then mapped through another multilayer perceptron to generate the win probability logits. This model structure is shown graphically in \autoref{fig:full-model}, and we initialize $\mathbf h_0$ randomly as is suggested by the PyTorch package and the literature \cite{PML}.

\begin{figure}
	\centering
	\begin{tikzpicture}[
			node distance = 1.5em and 3.5em,
			->,
			module/.style={ draw, rounded corners,
					inner sep=10pt, outer sep=5pt},
			every edge quotes/.style={fill=white, font=\small},
		]

		\node[module] (rnn1) {RNN};
		\node[module, right = of rnn1] (rnn2) {RNN};
		\node[module, right = of rnn2] (rnn3) {RNN};
		\node[right = of rnn3] (rnn4) {$\cdots$};

		\node[module, above = of rnn1] (mlpt1) {MLP\textsubscript{top}};
		\node[module, above = of rnn2] (mlpt2) {MLP\textsubscript{top}};
		\node[module, above = of rnn3] (mlpt3) {MLP\textsubscript{top}};

		\node[left = of rnn1] (h0) {$\mathbf h_0$};

		\node[above = of mlpt1] (x1) {$\mathbf x_1$};
		\node[above = of mlpt2] (x2) {$\mathbf x_2$};
		\node[above = of mlpt3] (x3) {$\mathbf x_3$};

		\node[module, below = 1cm of rnn1] (mlpb1) {MLP\textsubscript{bot}};
		\node[module, below = 1cm of rnn2] (mlpb2) {MLP\textsubscript{bot}};
		\node[module, below = 1cm of rnn3] (mlpb3) {MLP\textsubscript{bot}};

		\node[below = of mlpb1] (p1) {$\logit p_1$};
		\node[below = of mlpb2] (p2) {$\logit p_2$};
		\node[below = of mlpb3] (p3) {$\logit p_3$};

		\draw (h0) -- (rnn1);
		\draw (rnn1) edge["$\mathbf h_1$"] (rnn2);
		\draw (rnn2) edge["$\mathbf h_2$"] (rnn3);
		\draw (rnn3) edge["$\mathbf h_3$"] (rnn4);

		\draw (x1) -- (mlpt1);
		\draw (x2) -- (mlpt2);
		\draw (x3) -- (mlpt3);

		\draw (mlpt1) -- (rnn1);
		\draw (mlpt2) -- (rnn2);
		\draw (mlpt3) -- (rnn3);

		\draw (rnn1) edge["$\mathbf h_1$"] (mlpb1);
		\draw (rnn2) edge["$\mathbf h_2$"] (mlpb2);
		\draw (rnn3) edge["$\mathbf h_3$"] (mlpb3);

		\draw (mlpb1) -- (p1);
		\draw (mlpb2) -- (p2);
		\draw (mlpb3) -- (p3);
	\end{tikzpicture}

	\caption{Our network architecture.}
	\label{fig:full-model}
\end{figure}

We experiment with several different levels of complexity for the two multilayer perceptrons and the recurrent component. Precisely, we vary the number and dimension of hidden layers in the MLPs as well as the dimension of the hidden state $\mathbf h_t$. We also experiment with ``stacking'' RNN layers. In this configuration, the hidden states of first RNN layer serve as the input sequence for the second layer. We make our predictions from the second layer's hidden states. The five model ``sizes'' we test are given in \autoref{tbl:model-sizes}. Each of these sizes is tested with the simple RNN (\autoref{eqn:elman-rnn}) as well as the more-complicated gated and LSTM recurrent units. The performance of these models is discussed in the following section.

\begin{table}
	\begin{tabular}{r ccc}
		\hline
		Size        & RNN layers & $\mathbf h_t$ dimension & MLP hidden layers  \\
		\hline
		\texttt{xs} & 1          & 32                      & $\tuple{}$         \\
		\texttt{sm} & 1          & 64                      & $\tuple{64}$       \\
		\texttt{md} & 1          & 128                     & $\tuple{128}$      \\
		\texttt{lg} & 2          & 128                     & $\tuple{128, 128}$ \\
		\texttt{xl} & 4          & 128                     & $\tuple{128, 128}$ \\
		\hline
	\end{tabular}
	\caption{The five model sizes ranging from \texttt{xs} to \texttt{xl}.}
	\label{tbl:model-sizes}
\end{table}

\subsection{Estimating Network Parameters}

The model described in the previous subsection is quite complex, and our exposition of it left a big question unanswered: how do we find its parameters, i.e., the entries for the matrices in the MLP and recurrent layers (\autoref{eqn:mlp} and \autoref{eqn:elman-rnn})? The answer likes in a clever application of the chain rule known as backpropogation.

To illustrate how neural networks parameters are estimated, we work through the simple case of a multi-layer perceptron used for binary classification. We then describe how this process extends quite naturally to more complicated networks like our complete model in the previous section. More thorough descriptions of the backpropgation algorithm and fitting neural network parameters are given in \textcite[\S 13.4]{PML}, \textcite[\S 11.4]{ESL}, and \textcite[\S 10.7]{ISL}

To start, we recall the model equation for a multi-layer perceptron on some input $\mathbf z$---this time wrapped in a sigmoid function so that the model outputs can be interpreted as probabilities for binary classification:
\begin{equation}
	f(\mathbf z) = \sigma (C \sigma (A\mathbf z + \mathbf b)).
\end{equation}
Recall that $A$ and $C$ are matrices and $\mathbf b$ is a vector. We use the sigmoid function $\sigma$ as our a non-linear activation function. Our model outputs a single scalar that represents our estimate for the probability that $\mathbf z$ belongs in class $1$.

To find good choices of $A$, $\mathbf b$, and $C$, we minimize a \emph{loss function} $\ell$ with respect to a set of training data $\set{\mathbf z_i}$ with binary labels $\set{y_i}$. (Think of linear regression where we minimize the sum of squares error.) Put mathematically, we would like to solve the following optimization problem:
\begin{equation*}
	\argmin_{A, \mathbf b, C} \sum_i \ell(\hat f(\mathbf z_i), y_i)
\end{equation*}
In the context of our sport sequence win prediction, the training data are sequences of events and the labels are binary indicator variables representing the winning team, but for the sake of simplicity, we focus on the case when the inputs $\mathbf z_i$ are of a fixed dimension in this example.

In binary classification (our setting), we typically take the loss function to be the negative log likelihood
\begin{equation}
	\label{eqn:bce-loss}
	\ell(\hat f(\mathbf z), y) = - y \log \hat f(\mathbf z) - (1 - y) \log (1 - \hat f(\mathbf z)).
\end{equation}
In the deep learning literature, this loss function is known as \emph{binary cross-entropy}. Minimizing this loss function gives a maximum likelihood estimate for the parameters.

Unlike with ordinary least squares, there is unfortunately no nice closed-form solution for a neural network's parameters that minimizes this loss function. Even worse, this objective function is not necessarily convex, so we can't use off-the-shelf convex optimization techniques like with many other models that use maximum likelihood estimation \cite[\S 13.4]{PML}.
Instead, neural network parameters are chosen by what is known as \emph{gradient descent}: we make initial (usually random) parameter assignments and gradually update them with tiny nudges---each of which decreases the total loss by a small amount.

\begin{figure}
	\centering

	\begin{tikzpicture}[
			every edge quotes/.style = {below, font=\small}
		]
		\matrix [
			matrix of math nodes,
			column sep=3em,
			row sep=3em,
			nodes={anchor=center},
		] {
			                & |(A)|A        & |(b)| \mathbf b &              & |(C)| C       &               & |(y)| y    &                   \\
			|(x)| \mathbf z & |(t1)| \times & |(p)| +         & |(s)| \sigma & |(t2)| \times & |(s2)| \sigma & |(l)| \ell & |(L)| \text{loss} \\
		};

		\draw[->] (x) edge["$\mathbf v_0$"] (t1);
		\draw[->] (t1) edge["$\mathbf v_1$"] (p);
		\draw[->] (p) edge["$\mathbf v_2$"] (s);
		\draw[->] (s) edge["$\mathbf v_3$"] (t2);
		\draw[->] (t2) edge["$v_4$"] (s2);
		\draw[->] (s2) edge["$v_5$"] (l);
		\draw[->] (l) edge["$v_6$"] (L);

		\draw[->] (A) -- (t1);
		\draw[->] (b) -- (p);
		\draw[->] (C) -- (t2);
		\draw[->] (y) -- (l);
	\end{tikzpicture}
	\caption{The computational graph for the multilayer perceptron $f(\mathbf z) = \sigma(C \sigma(A\mathbf z +\mathbf b))$. Each leaf represents an input to our computation, and each internal node represents an operation applied to its parents. We compute the gradient of the loss function with respect to the models parameters by ``stepping backwards'' through this graph from $\ell$ to the parameter of interest.}
	\label{fig:mlp-computational-graph}
\end{figure}

In order to know how we should nudge our parameter assignments, we compute the partial derivatives of the loss function with respect to our parameter assignments using the backpropogation algorithm \cite[\S 13.3]{PML}. It has two steps.

During what is known as the \emph{forward pass}, we compute $\hat f(\mathbf z)$ and keep track of intermediate values
\begin{equation}
	\begin{aligned}
		\mathbf v_0 & = \mathbf z                                                     \\
		\mathbf v_1 & = A\mathbf v_0        &  & = A \mathbf z,                       \\
		\mathbf v_2 & = \mathbf v_1 + b     &  & = A\mathbf z  + \mathbf b,           \\
		\mathbf v_3 & = \sigma(\mathbf v_2) &  & = \sigma(A\mathbf z  + \mathbf b),   \\
		v_4         & = C \mathbf v_3       &  & = C \sigma(A\mathbf z  + \mathbf b), \\
		v_5         & = \sigma(v_4)         &  & = f(\mathbf z).
	\end{aligned}
\end{equation}
We then compute the loss
\begin{equation}
	\begin{aligned}
		v_6
		 & = \ell (v_5, y)                         \\
		 & = - y \log v_5 - (1 - y) \log (1 - v_5)
	\end{aligned}
\end{equation}
where $y$ is the corresponding binary label for $\mathbf z$.
In deep learning, this process is often shown graphically in what is called a computational graph. The computational graph for $f$ is shown in \autoref{fig:mlp-computational-graph}.

In the \emph{backward pass} we use these intermediate states and the chain rule to compute the derivative of the loss with respect to each of the models parameters.
For example, the gradient of $\ell$ loss with respect to $C$ is
\begin{equation}
	\begin{aligned}
		\frac{\partial \ell}{\partial C}
		 & = \frac{\partial v_6}{\partial C} \\
		 & = \frac{\partial v_4}{\partial C}
		\frac{\partial v_5}{\partial v_4}
		\frac{\partial \ell}{\partial v_5}.
	\end{aligned}
\end{equation}
It's often helpful to view this application of the chain rule as a backwards traversal in the computational graph (\autoref{fig:mlp-computational-graph}) from the loss to the parameters. At every step, we acquire an additional multiplicative term in our gradient expression.

The crucial observation is that each of these partial derivatives on the right-hand-side of the equation is easily computable given the intermediate values computed in the forward pass.
For the sigmoid non-linearity layer, one can verify that
\begin{equation}
	\frac{\partial v_5}{\partial v_4}
	= \frac{\partial \sigma(v_4)}{\partial v_4}
	= \sigma(v_4) \paren{1 - \sigma(v_4)}
	= v_5 \paren{1 - v_5}
\end{equation}
and for the binary cross entropy loss layer, we have
\begin{equation}
	\frac{\partial \ell}{\partial v_5} = \frac{1 - y}{1 - v_5} - \frac{y}{v_5}.
\end{equation}
Computing the partial derivative of the linear layer where we multiply by $C$ is not complicated, but it's a bit tedious and requires knowledge of high-dimensional vector calculus, so we omit it from our paper. It can be found in section 13.3.3.3 of \textcite{PML} along with derivations of the Jacobians for other common neural network layers. (Thankfully, in practice there are software packages to do the differentiation for us.)

Multiplying the partial derivatives we just computed gives $\frac{\partial \ell}{\partial C}$, and the quantities $\frac{\partial \ell}{\partial A}$ and $\frac{\partial \ell}{\partial \mathbf b}$ can be computed in an identical fashion. With these gradients in hand, we can nudge our parameter assignments a tiny amount:
\begin{equation}
	\begin{aligned}
		A'         & = A - \gamma \frac{\partial \ell}{\partial A},                 \\
		\mathbf b' & = \mathbf b - \gamma \frac{\partial \ell}{\partial \mathbf b}, \\
		C'         & = C - \gamma \frac{\partial \ell}{\partial C},
	\end{aligned}
\end{equation}
where $\gamma$ is some small constant called the \emph{learning rate}. We then repeat this process of computing gradients and updating parameters a large number of times.

To determine when to stop iterating, we withhold a portion of our data from training (typically 20 percent) and after each iteration of gradient descent (called an epoch), we evaluate our model's performance on this validation set.
If we reach a point in our gradient descent where our training loss continues to decrease, but the validation performance stops improving we say that our model has \emph{overfit}, i.e., learned patterns that are in the training data by chance and are not indicative of a real relationship.
This is the point at which we terminate training \cite[\S 10.7.2]{ISL}, \cite[\S 13.5.1]{PML}.

This was admittedly a toy example, and in practice, the process of computing gradients and updating parameter assignments is done automatically by a software library. There are many options, but we choose to use \texttt{pytorch} \cite{pytorch} for this project due to its modularity and its popularity in the research community. It's also common to use a more elaborate gradient descent technique than the one described here (we use Adam \cite{adam} to train our model) and to ``batch'' input in order to more efficiently utilize parallel computing during the training phase (we use a batch size of 500) \cite[\S 10.7.2]{ISL}.

One of the miracles of machine learning is how well this backpropogation algorithm scales \cite[\S 13.2.6]{PML}. The same ideas described here were used to train GPT-4, a model with on the order of $1$ \emph{trillion} parameters \cite{gpt4}. Although in principle, loss functions are non convex and gradient descent may get stuck in local minima, this tends not to happen in practice. It's still not entirely understood why this is the case \cite[\S 13.4]{PML}.
