\section{Neural Networks}

\begin{frame}{What even are they?}

	\pause

	\begin{definition}
		\smallskip
		\begin{center}
			\text{neural networks}
			= \text{linear functions} + \text{non-linearities}
		\end{center}
		\smallskip
	\end{definition}

	\pause \bigskip

	\begin{center}
		\includegraphics[width = .6 \textwidth]{salt-bae.jpg}
	\end{center}

\end{frame}

\begin{frame}{Introducing non-linearity}

	We want to estimate the relationship between $x \in \R^n$ and $y \in \R^m$.

	\bigskip \pause

	\begin{align*}
		\onslide<2->{g : \R^n \to \R^k      & \qquad & g(x)      & = A x + b \\}
		\smallskip
		\onslide<4->{\sigma : \R^k \to \R^k &        & \sigma(x) & = \frac{e^x}{1+e^x}\\}
		\smallskip
		\onslide<3->{h : \R^k \to \R^m      &        & h(x)      & = C x}
	\end{align*}

	\bigskip

	\begin{equation*}
		\R^n
		\ \onslide<2->{\underset{g}{\longrightarrow} \ \R^k}
		\ \onslide<4->{\underset{\sigma}{\longrightarrow} \ }
		\ \onslide<3->{\R^k \underset{h}{\longrightarrow} \ \R^m}
	\end{equation*}

	\bigskip

	\onslide<5->{
		\begin{equation*}
			\hat f = h \of \sigma \of g
		\end{equation*}
	}
\end{frame}

\begin{frame}{Why ``neural''?}
	\begin{center}
		\includegraphics[width = .7 \textwidth]{neural-net.png}
	\end{center}

	\pause

	\begin{center}
		``Multi-layer perceptron''
	\end{center}
\end{frame}

\begin{frame}
	\begin{center}
		\textbf{This seems contrived... why do we care?}
	\end{center}

	\bigskip \pause

	\begin{center}
		\begin{enumerate}[<+->]
			\item Expressiveness
			\item Trainability
		\end{enumerate}
	\end{center}
\end{frame}

\begin{frame}{Expressiveness}
	\textbf{Mathematically}

	For any continuous $f : K \subseteq \R^n \to \R^m$, there exists a sequence of functions
	\begin{equation*}
		f_i :
		\R^n
		\ \xrightarrow{Ax + b} \ \R^k
		\ \xrightarrow{\sigma(x)} \  \R^k
		\ \xrightarrow{Cx} \ \R^m
	\end{equation*}
	that converges uniformly to $f$.

	\pause \bigskip

	\textbf{In practice}

	\begin{itemize}
		\item ``Neural networks can approximate pretty much anything.''
	\end{itemize}
\end{frame}

\begin{frame}{Anything?}

	\pause

	\begin{center}
		\tikz{
			\node (image) {\includegraphics[height=1in]{mnist-3.png}};
			\node (matrix) [right = of image] {$\begin{pmatrix}
						.15    & \ldots & .03    \\
						\vdots & \ddots & \vdots \\
						.73    & \ldots & .08
					\end{pmatrix} \in \R^{w \times h}$};
			\draw[->] (image) -- (matrix);
		}

		\pause

		\tikz{
			\node (label) {$3$};
			\node (onehot) [right = of label] {$\begin{pmatrix}
						0      \\
						0      \\
						1      \\
						0      \\
						\vdots \\
						0
					\end{pmatrix} \in \R^{10}$};
			\draw[->] (label) -- (onehot);
		}
	\end{center}

	\note{Simple network (multi-layer perceptron) we've described gets above $95\%$ on this task.}
\end{frame}

\begin{frame}{Anything?}

	\qquad \includegraphics[height=1in]{brain-scans.jpg}
	\hfill	\pause
	\qquad \includegraphics[height=1in]{audio-signal.png}

	\pause \bigskip

	\begin{center}
		% TODO: come up with something funnier here?
		\texttt{"Carleton College is in Northfield, Minnesota"}

		$\downarrow$

		$\langle 54, 138, 13, 40, 190, 72 \rangle$
	\end{center}

	\pause \bigskip

	\note{Okay, these things are very expressive and very powerful, but we're sweeping a huge amount under the rug.}

	\begin{center}
		\textbf{How do we estimate the parameters?}
	\end{center}
\end{frame}

\begin{frame}{Training}

	Observations $x_i$, outcomes $y_i$, and a model $f_\beta(x_i)$.

	\pause \bigskip

	We want to find
	\begin{equation*}
		\argmin_\beta \sum_i \ell(f_\beta(x_i), y_i)
	\end{equation*}
	where $\ell$ is some "loss function".

	% \pause \bigskip
	%
	% Ideas:
	%
	% \begin{itemize}[<+->]
	% 	\item Least squares? ($\ell(a, b) = ||a - b||^2$)
	% 	\item Maximum likelihood estimation?
	% \end{itemize}
\end{frame}

\begin{frame}{Gradient Descent}

	\begin{center}
		\begin{equation*}
			\argmin_\beta \sum_i \ell(f_\beta(x_i), y_i)
		\end{equation*}

		\pause \bigskip

		\includegraphics[width=.4\textwidth]{gradient-descent.png}

		\vspace{.5in} \pause

		We can optimize our parameters by taking derivatives of the loss function.
	\end{center}

\end{frame}

\begin{frame}[fragile]{Backpropogation}

	\textbf{Key observation:} gradients of neural networks are easily computable.

	\begin{equation*}
		\argmin_\beta \sum_i \ell(f(x), y)
	\end{equation*}

	\bigskip \pause

	\textbf{Computational graph of multi-layer perceptron:}

	\begin{center}
		\begin{tikzpicture}[
				every edge quotes/.style = {below, font=\tiny}
			]
			\matrix [
			matrix of math nodes,
			column sep=2em,
			row sep=2em,
			nodes={anchor=center},
			] {
			& |(A)|A        & |(b)| b &              & |(C)| C       & |(y)| y    &         \\
			|(x)| x & |(t1)| \times & |(p)| + & |(s)| \sigma & |(t2)| \times & |(l)| \ell & |(L)| {\text{loss}} \\
			};

			\draw[->] (x) -- (t1);
			\draw[->] (t1) edge["$x_1$"] (p);
			\draw[->] (p) edge["$x_2$"] (s);
			\draw[->] (s) edge["$x_3$"] (t2);
			\draw[->] (t2) edge["$f(x)$"] (l);
			\draw[->] (l) -- (L);

			\draw[->] (A) -- (t1);
			\draw[->] (b) -- (p);
			\draw[->] (C) -- (t2);
			\draw[->] (y) -- (l);
		\end{tikzpicture}
	\end{center}

	% \medskip \pause

	% TODO: include this or not"
	% \begin{equation*}
	% 	\frac{\partial \ell}{\partial C}
	% 	= \frac{\partial \ell}{\partial f} \frac{\partial f}{\partial C}
	% \end{equation*}

	% TODO: do another example on the board

	\note{By keeping track of intermediate states, we can compute the gradient of the loss with respect to the parameters, and then update the parameters to move them closer to their optimal (loss-minimizing) states.}

\end{frame}

\begin{frame}[t]{Computational aside}

	In practice, modern neural networks have much more complicated computational graphs

	\pause

	\only<+>{
		\begin{center}
			\includegraphics[width=\textwidth]{transfomer.png}
		\end{center}

		GPT 4 has $\sim 1$ trillion parameters ($\sim 4$ TB just to store them)
	}

	% \pause \vspace{1in}
	%
	% Gradient descent scales surprisingly well, but...
	%
	% \pause \medskip
	%
	% ...updating parameters (i.e. computing gradients) takes a \textit{lot} of compute.
	%
	% \pause \smallskip
	%
	% \begin{itemize}
	% 	\item GPT 4 cost over \$100 million
	% 	      % altman: "it's more than that"
	% 	      % https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/
	% \end{itemize}
\end{frame}

\begin{frame}[t]{Evaluation}

	\pause

	Is our model $\hat f_\beta$ a good approximation of the real process $f$?

	\note{Or "is appropriate"}

	\pause \bigskip

	\textbf{Familiar tools}

	\begin{itemize}[<+->]
		\item Goodness of fit
		\item Diagnostic plots
		\item AIC, BIC, etc.
	\end{itemize}

	\pause \bigskip

	\textbf{Cross-validation}

	``Give it some data it hasn't seen before and see how well it does.''

	\medskip \pause

	\begin{enumerate}[<+->]
		\item Split your data into "training" and "validation" subsets.
		\item Train your model on \emph{only} the training data.
		\item Evaluate your model's performance on the "validation" data.
	\end{enumerate}
\end{frame}

\begin{frame}[t]{Evaluation}

	Is our model $\hat f_\beta$ a good approximation of the real process $f$?

	\begin{center}
		\includegraphics[width=.75\textwidth]{over-under-fitting.png}
	\end{center}

\end{frame}

\begin{frame}{Recap}
	\begin{itemize}[<+->]
		\item Neural nets $=$ matrix multiplication + non-linearity.
		\item They can approximate pretty much any function.
		\item We train them with backpropagation and gradient descent.
		\item We estimate their performance with cross-validated loss.
	\end{itemize}
\end{frame}
