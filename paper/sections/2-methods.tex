\section{Methods}

% \subsection{Support Vector Machines}
%
% % TODO: is it even worth describing this?
%
% computes 'max separating hyperplane' in data
%
% done by solving the following quadratic optimization problem
% \begin{equation}
% 	% TODO: 
% \end{equation}
%
% not used in our work because it can't model sequence data
%
% can model non-linear boundaries via basis expansion and mapping the input into a higher-dimensional space via a non-linear transformation. we comput the best linear boundary in this space and then map back down into the original space. this is like how in cubic regression we map do a basis expansion  $\R^1 \to \R^3$ by sending $(x) \mapsto (1, x, x^2, x^3)$. We then do linear regression with respect to this higher-dimensional representation of our data.
%
% this can be made efficient via the ``kernel trick''

\subsection{Neural Networks}

It's hard to define exactly what a neural network is: the term is used broadly to describe a large class of model characterized mostly by the method used to estimate the models' parameters: gradient descent via the backpropagation algorithm. In this paper, we focus on two specific types of neural networks: the multilayer perceptron and the recurrent neural network.

This is \emph{not} a comprehensive treatment. There are many more types of neural networks, each of which is well-suited for a specific task. A few examples are convolutional neural networks (which are commonly used for image-related tasks), transformers (which power natural-language models like GPT), and generative adversarial networks (which have found recent fame due their ability to generate deep-fakes). Chapter 11 of \textcite{ESL} and Part III of \textcite{PML} provide excellent and statistically-motivated introductions to the subject.

\subsection{The Multi-layer Perceptron}

The simplest non-trivial neural network is known as a \emph{multi-layer perceptron} or \emph{feedforward neural network} and can be thought of loosely as a non-linear sandwich on two slices of linear bread. More precisely, to model a function $f : \R^n \to \R^m$, we introduce an intermediate space $\R^k$---typically called the \emph{hidden layer}. The dimension of this space is a model hyperparameter. We first map our inputs into this intermediate space via a linear function $x \mapsto Ax + b$ where $A$ is a $k \times n$ matrix and $b$ is a $k$-dimensional vector. We then apply a non-linear function $\sigma$ element-wise to our values in the intermediate space $\R^k$ before mapping back down to our output space $\R^m$ via a linear transformation $x \mapsto Cx$ where $C$ is am $m \times k$ matrix. In total, the multi-layer perceptron model equation is
\begin{equation}
	\label{eqn:mlp}
	f(x) = C \, \sigma(Ax + b).
\end{equation}

In the world of deep learning, we typically refer to the matrices $A$ and $C$ as \emph{weights} and the vector $b$ as \emph{bias}. The function $\sigma$ is known as the \emph{activation function}. Historically, the most popular choices for $\sigma$ have been the sigmoid function
\begin{equation}
	\sigma(x) = \frac{e^x}{1 + e^x}
\end{equation}
and the hyperbolic tangent, $\tanh$. More recently the ``recified linear unit'' function
\begin{equation}
	{\rm ReLu}(x) = \max(x, 0)
\end{equation}
has become popular due to its cheap-to-compute gradient. (The following section will reveal why this is relevant.)

\begin{figure}
	\centering
	\begin{tikzpicture}
		\graph[
		math nodes,
		layered layout, grow=right, level sep=6em, sibling sep=3em,
		edges={->, gray}, nodes={inner sep = 1em}
		]{
		subgraph I_n [V={x_3, x_2, x_1}, name=input] -- [complete bipartite]
		subgraph I_n [V={z_4, z_3, z_2, z_1}, name=hidden] (hidden) -- [complete bipartite]
		subgraph I_n [V={y_2, y_1}, name=output]
		};

		\node[above = 1em of hidden z_1] (hidden label) {\underline{Hidden layer}};
		\node[left = of hidden label] {\underline{Input layer}};
		\node[right = of hidden label] {\underline{Output layer}};
	\end{tikzpicture}

	\caption{A multi-layer perceptron with $n = 3$, $k = 4$, and $m = 2$.  We first map our input into a ``hidden layer'' via a linear transformation. In the internal hidden layer, we apply a non-linear function element-wise before applying another linear transformation to map into our output space. Mathematically, each ``neuron'' in the hidden layer takes on the value $z_i = \sigma(a_{i1} x_1 + \cdots + a_{in} x_n + b_i)$ and the ``neurons'' in the output layer take the values $y_i = c_{i1} z_1 + \cdots + w_{ik} z_k$. The ``weights'' $\set{a_{ij}} \cup \set{b_{ij}}$ and ``biases'' $\set{b_i}$ are the model's parameters.}
	\label{fig:mlp-diagram}
\end{figure}

This simple model is what is known as a \emph{universal function approximator}. In layman's terms, this means that for any function $g : \R^n \to \R^m$, there exist choices for $k$, $A$, $b$ and $C$ such that the multi-layer perceptron can approximate $g$ to any desired level of precision. More mathematically, for any continuous $g : K \to \R^m$ where $K \subseteq R^n$ is compact, there exists a sequence of functions $f_i$ of the form in \autoref{eqn:mlp} that uniformly converges to $g$. This property is true regardless of the choice of activation function, with the only condition being that $\sigma$ not be polynomial.

Although this universal flexibility in theory only requires a single hidden layer, the above theorem places no bound on $k$---the hidden layer's dimension. In practice, one can (and often does) increase the network's ``depth'' by including multiple hidden layers in order to limit the networks ``width''. The construction extends quite naturally from the setting with a single hidden layer: between each layer, we apply a linear transformation $x \mapsto Ax + b$. At each hidden layer, we apply an element-wise non-linearity $\sigma$.

The name ``neural network'' makes a bit of sense when the input, hidden, and output spaces are laid out as in \autoref{fig:mlp-diagram}. Here, the elements of each space are thought of as ``neurons'' with the weights representing the strength of each neural connection.
This connection with the human brain was apparently the inspiration for the conception of the multilayer perceptron in 1958, however the terms ``perceptron'' and ``neural network'' often receive criticism for exaggerating what is at best a tenuous connection.

\subsection{Estimating Network Parameters}

In the previous section, we established that the simple multilayer perceptron can approximate a wide class of functions with the right choices of $k$, $A$, $b$, and $C$. But how do we find these parameters?

Framing this in a more classical statistical setting, we would like to find $A$, $b$, and $C$ such that the output of our model $\hat f(x)$ is as close as possible to the true value $y$. If we have a set of observations $\set{x_i}$ with labels $\set{y_i}$, we would like to solve the following minimization problem:
\begin{equation*}
	\argmin_\beta \sum_i \ell(\hat f(x_i), y_i)
\end{equation*}
where $\ell$ is some context dependent ``loss function'' and $\beta$ is the set of all our models parameters. The familiar choice from regression is the squared error
\begin{equation}
	\ell(\hat f(x), y) = \norm{\hat f(x) - y}^2.
\end{equation}
In binary classification (the setting for our win-prediction), one typically uses \emph{binary cross-entropy loss}, defined as
\begin{equation}
	\label{eqn:bce-loss}
	\ell(\hat f(x), y) = y \log \hat f(x) + (1 - y) \log (1 - \hat f(x))
\end{equation}
where $y \in \set{0, 1}$ is our binary label and $\hat f(x)$ is our predicted probability that $x$ belongs to the class $1$.
This loss function has a close relationship to maximum likelihood estimation. In fact, it turns out that minimizing BCE loss yields a selection of parameters with maximum likelihood.

Unfortunately, unlike in linear regression, there is no nice closed-form solution for a neural network's parameters.
In many cases, we do not even want a global minimum for the loss function because the number of parameters in neural networks is so large that the models tend to over-fit.
Instead, we make initial (usually random) parameter assignments and gradually update them with tiny nudges---each of which decreases the total loss by a small amount.
To do this, we compute the gradient of the loss function with respect to the parameters through a clever application of the chain rule known as backpropogation.

To demonstrate how this works, we return to the multilayer perceptron. During what is known as the ``forward pass'' in which we compute $\hat f(x)$, we keep track of intermediate states
\begin{equation}
	\begin{aligned}
		x_1 & = Ax,                          \\
		x_2 & = Ax  + b,                     \\
		x_3 & = \sigma(Ax  + b),             \\
		x_4 & = f(x) = C \, \sigma(Ax  + b).
	\end{aligned}
\end{equation}
In the ``backward pass'' we use these intermediate states and the chain rule to compute the derivative of the loss with respect to each of the models parameters.
For example, the gradient of the MLP loss with respect to $C$ is
\begin{equation*}
	\frac{\partial \ell}{\partial C}
	= \frac{\partial \ell}{\partial x_4}
	\frac{\partial x_4}{\partial C}.
\end{equation*}
The crucial observation is that each of these partial derivatives is easily computable given the intermediate values computed in the forward pass. For example,
\begin{equation}
	\frac{\partial x_4}{\partial C} = x_3.
\end{equation}
% and when we use the BCE loss (\autoref{eqn:bce-loss}),
% \begin{equation}
% 	% TODO: compute this
% 	\frac{\partial \ell}{\partial x_4} =
% \end{equation}
Derivatives with respect to the other two parameter groups follow from the same process:
\begin{equation*}
	\frac{\partial \ell}{\partial b}
	= \frac{\partial \ell}{\partial x_4}
	\frac{\partial x_4}{\partial x_3}
	\frac{\partial x_3}{\partial x_2}
	\frac{\partial x_2}{\partial b},
\end{equation*}
\begin{equation*}
	\frac{\partial \ell}{\partial A}
	= \frac{\partial \ell}{\partial x_4}
	\frac{\partial x_4}{\partial x_3}
	\frac{\partial x_3}{\partial x_2}
	\frac{\partial x_2}{\partial x_1}
	\frac{\partial x_1}{\partial A}.
\end{equation*}

This process of computing gradients is known as \emph{backpropogation} and is the basis for training all neural networks. When we write out a neural network as a computational graph (\autoref{fig:mlp-computational-graph}), we can view backpropogation as a backwards traversal from the loss to the parameters, where at every step in the graph, we acquire an additional term in our gradient expression.

\begin{figure}
	\centering
	\begin{tikzpicture}[
			every edge quotes/.style = {below, font=\small}
		]
		\matrix [
		matrix of math nodes,
		column sep=3em,
		row sep=3em,
		nodes={anchor=center},
		] {
		& |(A)|A        & |(b)| b &              & |(C)| C       & |(y)| y    &         \\
		|(x)| x & |(t1)| \times & |(p)| + & |(s)| \sigma & |(t2)| \times & |(l)| \ell & |(L)| {\text{loss}} \\
		};

		\draw[->] (x) -- (t1);
		\draw[->] (t1) edge["$x_1$"] (p);
		\draw[->] (p) edge["$x_2$"] (s);
		\draw[->] (s) edge["$x_3$"] (t2);
		\draw[->] (t2) edge["$x_4$"] (l);
		\draw[->] (l) -- (L);

		\draw[->] (A) -- (t1);
		\draw[->] (b) -- (p);
		\draw[->] (C) -- (t2);
		\draw[->] (y) -- (l);
	\end{tikzpicture}
	\caption{The computational graph for the multilayer perceptron $C \sigma(Ax + b)$. Each leaf represents an input to our computation, and each internal node represents an operation applied to its children. We compute the gradient of the loss function with respect to the models parameters by ``stepping backwards'' through this graph from $\ell$ to the parameter of interest.}
	\label{fig:mlp-computational-graph}
\end{figure}

With these gradients in hand, we update our original guesses
\begin{equation}
	\begin{aligned}
		A' & = A - \gamma \frac{\partial \ell}{\partial A}, \\
		b' & = b - \gamma \frac{\partial \ell}{\partial b}, \\
		C' & = C - \gamma \frac{\partial \ell}{\partial C},
	\end{aligned}
\end{equation}
and by repeating this process enough times, one arrives as what is typically a good parameter estimate. In practice, this process of computing gradients and updating parameter assignments is done automatically by a software library (we use \texttt{pytorch} \cite{pytorch}).

One of the miracles of machine learning is how well this process scales. The same ideas described here were used to train GPT-4, a model with on the order of $1$ \emph{trillion} parameters.

\subsection{Recurrent Neural Networks}

Until now, we haven't discussed how to model sequences with neural networks. This requires consideration. For example, in the context of our data set, the win probability at time $t$ depends on much more than just the immediately-preceding event. In fact, $p_t$ depends on \emph{all} the preceding events $x_1, \ldots, x_t$.

In order to enable past events to influence present predictions, one common approach is to maintain an internal hidden state $h_t$ that one can think of as capturing the state of the sequence at time $t$. We continually update $h_t$ with new events as is shown in  \autoref{fig:simple-rnn}, and it is from this hidden state $h_t$ that predictions are made. This approach to sequence modeling is known as a \emph{recurrent neural network} and, until recently, been the main way that sequences are handled with deep learning.

\begin{figure}
	\centering
	\begin{tikzpicture}[
			node distance = 1.5em and 4em,
			->,
			module/.style={ draw, rounded corners,
					inner sep=10pt, outer sep=5pt},
			every edge quotes/.style={fill=white, font=\small},
		]

		\node[module] (rnn1) {RNN};
		\node[module, right = of rnn1] (rnn2) {RNN};
		\node[module, right = of rnn2] (rnn3) {RNN};

		\node[left = of rnn1] (h0) {};
		\node[right = of rnn3] (h1) {};

		\node[above = of rnn1] (x1) {$x_t$};
		\node[above = of rnn2] (x2) {$x_{t+1}$};
		\node[above = of rnn3] (x3) {$x_{t+2}$};

		\draw[->] (x1) -- (rnn1);
		\draw[->] (x2) -- (rnn2);
		\draw[->] (x3) -- (rnn3);
		\draw[->] (h0) edge["$h_{t-1}$"] (rnn1);
		\draw[->] (rnn1) edge["$h_t$"] (rnn2);
		\draw[->] (rnn2) edge["$h_{t+1}$"] (rnn3);
		\draw[->] (rnn3) edge["$h_{t+2}$"] (h1);
	\end{tikzpicture}

	\caption{A simple recurrent neural network.}
	\label{fig:simple-rnn}
\end{figure}

The RNN internals are similar to the multilayer perceptron: we update the hidden state via a non-linear function applied to a linear combination of $x_t$ and the previous hidden state $h_{t-1}$. Precisely,
\begin{equation}
	\label{eqn:elman-rnn}
	h_t = \sigma(A x_t + Bh_{t-1} + c)
\end{equation}
where $\sigma$ is some non-linear function---we use the hyperbolic tangent because that is the default option in the \texttt{pytorch} package we use to implement our model. The model in \autoref{eqn:elman-rnn} is known as an Elman RNN.

We also use two other forms of recurrent neural networks known as Gated Recurrent Units and Long Short Term Memory. These employ more complex internal logic that allow gradients to ``flow'' backwards more cleanly and enable events further in the past to have influence on the current hidden state. Explaining the precise internals of these two RNN types is outside the scope of this paper. See \textcite[\S 15.2.7]{PML} for a complete exposition.

\subsection{The Complete Sport Sequence Model}

With all the pieces built up, we now describe our complete model.

The raw event data is first fed into a multilayer perceptron, then passed into the recurrent layer to generate the hidden game state $h_t$. This game state is then mapped through another multilayer perceptron to generate the win probability logits. This model structure is shown graphically in \autoref{fig:full-model}.

\begin{figure}
	\centering
	\begin{tikzpicture}[
			node distance = 1.5em and 3.5em,
			->,
			module/.style={ draw, rounded corners,
					inner sep=10pt, outer sep=5pt},
			every edge quotes/.style={fill=white, font=\small},
		]

		\node[module] (rnn1) {RNN};
		\node[module, right = of rnn1] (rnn2) {RNN};
		\node[module, right = of rnn2] (rnn3) {RNN};
		\node[right = of rnn3] (rnn4) {$\cdots$};

		\node[module, above = of rnn1] (mlpt1) {MLP\textsubscript{top}};
		\node[module, above = of rnn2] (mlpt2) {MLP\textsubscript{top}};
		\node[module, above = of rnn3] (mlpt3) {MLP\textsubscript{top}};

		\node[left = of rnn1] (h0) {$h_0$};

		\node[above = of mlpt1] (x1) {$x_1$};
		\node[above = of mlpt2] (x2) {$x_2$};
		\node[above = of mlpt3] (x3) {$x_3$};

		\node[module, below = 1cm of rnn1] (mlpb1) {MLP\textsubscript{bot}};
		\node[module, below = 1cm of rnn2] (mlpb2) {MLP\textsubscript{bot}};
		\node[module, below = 1cm of rnn3] (mlpb3) {MLP\textsubscript{bot}};

		\node[below = of mlpb1] (p1) {$\logit p_1$};
		\node[below = of mlpb2] (p2) {$\logit p_2$};
		\node[below = of mlpb3] (p3) {$\logit p_3$};

		\draw (h0) -- (rnn1);
		\draw (rnn1) edge["$h_1$"] (rnn2);
		\draw (rnn2) edge["$h_2$"] (rnn3);
		\draw (rnn3) edge["$h_3$"] (rnn4);

		\draw (x1) -- (mlpt1);
		\draw (x2) -- (mlpt2);
		\draw (x3) -- (mlpt3);

		\draw (mlpt1) -- (rnn1);
		\draw (mlpt2) -- (rnn2);
		\draw (mlpt3) -- (rnn3);

		\draw (rnn1) edge["$h_1$"] (mlpb1);
		\draw (rnn2) edge["$h_2$"] (mlpb2);
		\draw (rnn3) edge["$h_3$"] (mlpb3);

		\draw (mlpb1) -- (p1);
		\draw (mlpb2) -- (p2);
		\draw (mlpb3) -- (p3);
	\end{tikzpicture}

	\caption{Our network architecture.}
	\label{fig:full-model}
\end{figure}

We experiment with several different levels of complexity for the two multilayer perceptrons and the recurrent component. Precisely, we vary the number of hidden layers in the MLPs as well as the dimension of the hidden state $h_t$. We also experiment with ``stacking'' RNN layers. In this configuration, the hidden states of first RNN layer serve as the input sequence for the second layer. We make our predictions from the second layer's hidden states. The five model ``sizes'' we test are given in \autoref{tbl:model-sizes}. Each of these sizes is tested with the simple Elman RNN as well as the more-complicated gated and LSTM recurrent units.

\begin{table}
	\begin{tabular}{r ccc}
		\hline
		Size        & RNN layers & $h_t$ dimension & MLP hidden layers  \\
		\hline
		\texttt{xs} & 1          & 32              & $\tuple{}$         \\
		\texttt{sm} & 1          & 64              & $\tuple{64}$       \\
		\texttt{md} & 1          & 128             & $\tuple{128}$      \\
		\texttt{lg} & 2          & 128             & $\tuple{128, 128}$ \\
		\texttt{xl} & 4          & 128             & $\tuple{128, 128}$ \\
		\hline
	\end{tabular}
	\caption{The five model sizes ranging from \texttt{xs} to \texttt{xl}.}
	\label{tbl:model-sizes}
\end{table}
